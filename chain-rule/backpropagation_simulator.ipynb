{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Backpropagation Simulator\n",
    "## For AP Calculus Students Learning Neural Networks\n",
    "\n",
    "This notebook demonstrates how the chain rule powers neural network learning through interactive visualizations of backpropagation.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how neural networks use the chain rule to learn\n",
    "- See backpropagation in action with real calculations\n",
    "- Connect your calculus knowledge to artificial intelligence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "Run this cell first to load all the tools we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully!\")\n",
    "print(\"\\nReady to explore backpropagation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: The Simple Network from Your Worksheet\n",
    "\n",
    "Let's recreate the exact network from your discovery worksheet:\n",
    "- **Hidden layer:** h = 3x + w‚ÇÅ\n",
    "- **Output layer:** y = 2h + w‚ÇÇ\n",
    "- **Error:** E = ¬Ω(y - target)¬≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork:\n",
    "    \"\"\"\n",
    "    A simple 2-layer network matching the worksheet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, w1=1.0, w2=3.0):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.history = {'w1': [w1], 'w2': [w2], 'error': []}\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        self.x = x\n",
    "        self.h = 3 * x + self.w1\n",
    "        self.y = 2 * self.h + self.w2\n",
    "        self.target = target\n",
    "        self.error = 0.5 * (self.y - target) ** 2\n",
    "        return self.y, self.error\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backward pass: Calculate gradients using the chain rule.\n",
    "        This is where the magic happens!\n",
    "        \"\"\"\n",
    "        # Gradient of error with respect to output\n",
    "        dE_dy = (self.y - self.target)\n",
    "        \n",
    "        # Gradient for w2: dE/dw2 = dE/dy * dy/dw2\n",
    "        dy_dw2 = 1\n",
    "        self.dE_dw2 = dE_dy * dy_dw2\n",
    "        \n",
    "        # Gradient for w1: dE/dw1 = dE/dy * dy/dh * dh/dw1 (chain rule!)\n",
    "        dy_dh = 2\n",
    "        dh_dw1 = 1\n",
    "        self.dE_dw1 = dE_dy * dy_dh * dh_dw1\n",
    "        \n",
    "        return self.dE_dw1, self.dE_dw2\n",
    "    \n",
    "    def update_weights(self, learning_rate=0.1):\n",
    "        \"\"\"Update weights using gradient descent\"\"\"\n",
    "        self.w1 = self.w1 - learning_rate * self.dE_dw1\n",
    "        self.w2 = self.w2 - learning_rate * self.dE_dw2\n",
    "        self.history['w1'].append(self.w1)\n",
    "        self.history['w2'].append(self.w2)\n",
    "        self.history['error'].append(self.error)\n",
    "    \n",
    "    def train_step(self, x, target, learning_rate=0.1):\n",
    "        \"\"\"One complete training step\"\"\"\n",
    "        self.forward(x, target)\n",
    "        self.backward()\n",
    "        self.update_weights(learning_rate)\n",
    "        return self.error\n",
    "\n",
    "print(\"‚úì Network class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's verify your worksheet calculations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network with worksheet values\n",
    "net = SimpleNetwork(w1=1.0, w2=3.0)\n",
    "\n",
    "# Run forward pass\n",
    "x_input = 2\n",
    "target_output = 20\n",
    "y_pred, initial_error = net.forward(x_input, target_output)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INITIAL NETWORK STATE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input: x = {x_input}\")\n",
    "print(f\"Target: {target_output}\")\n",
    "print(f\"Weights: w1 = {net.w1}, w2 = {net.w2}\")\n",
    "print(\"\\nFORWARD PASS:\")\n",
    "print(f\"  h = 3({x_input}) + {net.w1} = {net.h}\")\n",
    "print(f\"  y = 2({net.h}) + {net.w2} = {y_pred}\")\n",
    "print(f\"  Error = 0.5({y_pred} - {target_output})¬≤ = {initial_error}\")\n",
    "\n",
    "# Calculate gradients\n",
    "dE_dw1, dE_dw2 = net.backward()\n",
    "\n",
    "print(\"\\nBACKWARD PASS (Chain Rule):\")\n",
    "print(f\"  dE/dw2 = {dE_dw2:.2f}\")\n",
    "print(f\"  dE/dw1 = {dE_dw1:.2f}\")\n",
    "\n",
    "print(\"\\nDo these match your worksheet? ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Watch the Network Learn!\n",
    "\n",
    "Now let's train the network for multiple steps and watch it improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset network\n",
    "net = SimpleNetwork(w1=1.0, w2=3.0)\n",
    "\n",
    "# Train for 20 steps\n",
    "print(\"Training Progress:\")\n",
    "print(f\"{'Step':<6} {'w1':<10} {'w2':<10} {'Output':<10} {'Error':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for step in range(20):\n",
    "    error = net.train_step(x_input, target_output, learning_rate=0.1)\n",
    "    y_current, _ = net.forward(x_input, target_output)\n",
    "    \n",
    "    if step % 5 == 0 or step == 19:\n",
    "        print(f\"{step:<6} {net.w1:<10.4f} {net.w2:<10.4f} {y_current:<10.4f} {error:<10.6f}\")\n",
    "\n",
    "print(f\"\\n‚úì Network converged to target value {target_output}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Weight evolution\n",
    "ax1 = axes[0]\n",
    "iterations = range(len(net.history['w1']))\n",
    "ax1.plot(iterations, net.history['w1'], 'b-', linewidth=2, label='w1', marker='o')\n",
    "ax1.plot(iterations, net.history['w2'], 'r-', linewidth=2, label='w2', marker='s')\n",
    "ax1.set_xlabel('Training Step', fontsize=12)\n",
    "ax1.set_ylabel('Weight Value', fontsize=12)\n",
    "ax1.set_title('Weight Changes During Training', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Error reduction\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(1, len(net.history['error']) + 1), net.history['error'], \n",
    "         'g-', linewidth=2, marker='o')\n",
    "ax2.set_xlabel('Training Step', fontsize=12)\n",
    "ax2.set_ylabel('Error', fontsize=12)\n",
    "ax2.set_title('Error Decreases Over Time', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì The chain rule guided the network to the solution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Interactive Exploration\n",
    "\n",
    "**Your Turn!** Try changing the weights below and see how it affects the error landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these values and re-run the cell!\n",
    "test_w1 = 1.0  # Try different values\n",
    "test_w2 = 3.0  # Try different values\n",
    "\n",
    "net_test = SimpleNetwork(w1=test_w1, w2=test_w2)\n",
    "y_pred, error = net_test.forward(x_input, target_output)\n",
    "dE_dw1, dE_dw2 = net_test.backward()\n",
    "\n",
    "print(f\"Current State:\")\n",
    "print(f\"  w1={test_w1}, w2={test_w2}\")\n",
    "print(f\"  Output: {y_pred:.2f} (target: {target_output})\")\n",
    "print(f\"  Error: {error:.4f}\")\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"  dE/dw1 = {dE_dw1:.4f} ‚Üí {'decrease w1' if dE_dw1 > 0 else 'increase w1'}\")\n",
    "print(f\"  dE/dw2 = {dE_dw2:.4f} ‚Üí {'decrease w2' if dE_dw2 > 0 else 'increase w2'}\")\n",
    "\n",
    "# Visualize error landscape\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# w2 error surface\n",
    "w2_range = np.linspace(test_w2 - 5, test_w2 + 5, 100)\n",
    "errors_w2 = []\n",
    "for w2 in w2_range:\n",
    "    net_tmp = SimpleNetwork(w1=test_w1, w2=w2)\n",
    "    _, err = net_tmp.forward(x_input, target_output)\n",
    "    errors_w2.append(err)\n",
    "\n",
    "axes[0].plot(w2_range, errors_w2, 'r-', linewidth=2)\n",
    "axes[0].plot(test_w2, error, 'ro', markersize=12, label='Your position')\n",
    "axes[0].set_xlabel('w2 value')\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[0].set_title(f'Error vs w2 (gradient = {dE_dw2:.2f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# w1 error surface\n",
    "w1_range = np.linspace(test_w1 - 5, test_w1 + 5, 100)\n",
    "errors_w1 = []\n",
    "for w1 in w1_range:\n",
    "    net_tmp = SimpleNetwork(w1=w1, w2=test_w2)\n",
    "    _, err = net_tmp.forward(x_input, target_output)\n",
    "    errors_w1.append(err)\n",
    "\n",
    "axes[1].plot(w1_range, errors_w1, 'b-', linewidth=2)\n",
    "axes[1].plot(test_w1, error, 'bo', markersize=12, label='Your position')\n",
    "axes[1].set_xlabel('w1 value')\n",
    "axes[1].set_ylabel('Error')\n",
    "axes[1].set_title(f'Error vs w1 (gradient = {dE_dw1:.2f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Extension - Nonlinear Activation\n",
    "\n",
    "Real neural networks use **nonlinear activation functions**. The most common is the **sigmoid function**:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Its derivative is: $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "\n",
    "This adds one more link to the chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearNetwork:\n",
    "    \"\"\"\n",
    "    Network with sigmoid activation:\n",
    "    h = sigmoid(3*x + w1)\n",
    "    y = 2*h + w2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, w1=1.0, w2=3.0):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.history = {'w1': [w1], 'w2': [w2], 'error': []}\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        s = NonlinearNetwork.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        self.x = x\n",
    "        self.z = 3 * x + self.w1\n",
    "        self.h = self.sigmoid(self.z)\n",
    "        self.y = 2 * self.h + self.w2\n",
    "        self.target = target\n",
    "        self.error = 0.5 * (self.y - target) ** 2\n",
    "        return self.y, self.error\n",
    "    \n",
    "    def backward(self):\n",
    "        dE_dy = (self.y - self.target)\n",
    "        \n",
    "        # For w2 (same as before)\n",
    "        dy_dw2 = 1\n",
    "        self.dE_dw2 = dE_dy * dy_dw2\n",
    "        \n",
    "        # For w1 (now with sigmoid derivative!)\n",
    "        dy_dh = 2\n",
    "        dh_dz = self.sigmoid_derivative(self.z)  # New!\n",
    "        dz_dw1 = 1\n",
    "        self.dE_dw1 = dE_dy * dy_dh * dh_dz * dz_dw1\n",
    "        \n",
    "        return self.dE_dw1, self.dE_dw2\n",
    "    \n",
    "    def update_weights(self, learning_rate=0.1):\n",
    "        self.w1 = self.w1 - learning_rate * self.dE_dw1\n",
    "        self.w2 = self.w2 - learning_rate * self.dE_dw2\n",
    "        self.history['w1'].append(self.w1)\n",
    "        self.history['w2'].append(self.w2)\n",
    "        self.history['error'].append(self.error)\n",
    "    \n",
    "    def train_step(self, x, target, learning_rate=0.1):\n",
    "        self.forward(x, target)\n",
    "        self.backward()\n",
    "        self.update_weights(learning_rate)\n",
    "        return self.error\n",
    "\n",
    "print(\"‚úì Nonlinear network ready!\")\n",
    "print(\"\\nNotice: The chain for w1 now has 4 terms instead of 3:\")\n",
    "print(\"  dE/dw1 = dE/dy √ó dy/dh √ó dh/dz √ó dz/dw1\")\n",
    "print(\"           ‚Üë sigmoid derivative added here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linear vs nonlinear\n",
    "net_linear = SimpleNetwork(w1=1.0, w2=3.0)\n",
    "net_nonlinear = NonlinearNetwork(w1=1.0, w2=3.0)\n",
    "\n",
    "for _ in range(30):\n",
    "    net_linear.train_step(x_input, target_output, learning_rate=0.5)\n",
    "    net_nonlinear.train_step(x_input, target_output, learning_rate=0.5)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "iterations = range(len(net_linear.history['error']))\n",
    "plt.plot(iterations, net_linear.history['error'], 'b-', \n",
    "         linewidth=2, label='Linear', marker='o', markersize=4)\n",
    "plt.plot(iterations, net_nonlinear.history['error'], 'r-',\n",
    "         linewidth=2, label='Nonlinear (Sigmoid)', marker='s', markersize=4)\n",
    "plt.xlabel('Training Step', fontsize=12)\n",
    "plt.ylabel('Error', fontsize=12)\n",
    "plt.title('Linear vs Nonlinear: Both Use the Chain Rule!', fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Both networks learn, but the chain is longer for the nonlinear one!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: What You've Learned\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "1. **Backpropagation = Chain Rule**\n",
    "   - Every weight update uses the chain rule\n",
    "   - Longer networks = longer chains of derivatives\n",
    "   - Same principle scales from 2 weights to billions!\n",
    "\n",
    "2. **Gradients Show the Direction**\n",
    "   - Negative gradient ‚Üí increase weight\n",
    "   - Positive gradient ‚Üí decrease weight\n",
    "   - Learning rate controls step size\n",
    "\n",
    "3. **Your Calculus Matters**\n",
    "   - Every chain rule problem builds AI intuition\n",
    "   - Modern AI wouldn't exist without this math\n",
    "   - You're learning the foundation of machine learning!\n",
    "\n",
    "### üí° The Big Picture\n",
    "\n",
    "When you compute $\\frac{d}{dx}[f(g(x))]$, you're practicing the exact operation that teaches:\n",
    "- ChatGPT to write\n",
    "- Self-driving cars to navigate\n",
    "- Medical AI to diagnose diseases\n",
    "- Image generators to create art\n",
    "\n",
    "**The chain rule isn't just homework‚Äîit's the engine of artificial intelligence!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
